import findspark
findspark.init()
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.functions import col, asc,desc
from pyspark.sql import SQLContext
from pyspark.mllib.stat import Statistics
import pandas as pd
from pyspark.sql.functions import udf
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler,StandardScaler
from pyspark.ml import Pipeline
from sklearn.metrics import confusion_matrix
spark=SparkSession.builder \
.master ("local[*]")\
.appName("part3")\
.getOrCreate()
sc=spark.sparkContext
sqlContext=SQLContext(sc)
#sc=spark.sparkContext
sqlContext=SQLContext(sc)
SparkSession.builder.getOrCreate()
SparkSession.builder.getOrCreate()
#sc=spark.sparkContext
sqlContext=SQLContext(sc)
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.functions import col, asc,desc
from pyspark.sql import SQLContext
from pyspark.mllib.stat import Statistics
from pyspark.sql.functions import udf
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler,StandardScaler
from pyspark.ml import Pipeline
spark=SparkSession.builder \
.master ("local[*]")\
.appName("part3")\
.getOrCreate()
data_path = 'data/diabetes.csv'
df_diabetes=spark.read \
.option("header","True")\
.option("inferSchema","True")\
.option("sep",",")\
.csv(data_path)
#####################################################
print("There are", df_diabetes.count(), "rows", len(df_diabetes.columns),
"columns in the data.")
df_diabetes.show(4)
my_columns = ["Outcome", "Glucose", "BloodPressure","BMI"]
df_diabetes.select(my_columns).show(4)
my_columns = ["Outcome", "Glucose", "BloodPressure","BMI"]
df_diabetes.select(my_columns).show(4)
df = correlation_df(df=df_diabetes, target_var='Outcome',
feature_cols=my_cols,
method='spearman')
df.show()
def correlation_df(df, target_var, feature_cols, method):
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.stat import Correlation
# Assemble features into a vector
target_var = [target_var]
feature_cols = feature_cols
df_cor = df.select(target_var+feature_cols)
assembler = VectorAssembler(inputCols=target_var+feature_cols, outputCol="features")
df_cor = assembler.transform(df_cor)
# Calculate correlation matrix
correlation_matrix = Correlation.corr(df_cor, "features", method=method).head()
# Extract the correlation coefficient between target and each feature
target_corr_list = [correlation_matrix[0][i, 0] for i in range(len(feature_cols)+1)][1:]
# Create a DataFrame with target variable, feature names, and correlation coefficients
correlation_data = [(feature_cols[i], float(target_corr_list[i])) for i in range(len(feature_cols))]
correlation_df = spark.createDataFrame(correlation_data, ["feature", "correlation"])
# Print the result
return correlation_df
df = correlation_df(df=df_diabetes, target_var='Outcome',
feature_cols=my_cols,
method='spearman')
df.show()
df = correlation_df(df=df_diabetes, target_var='Outcome',
feature_cols=my_columns,
method='spearman')
df.show()
df = correlation_df(df=df_diabetes, target_var='Outcome',
feature_cols=my_columns,
method='spearman')
df.show()
target = 'Outcome'
inputs = ["Glucose", "BloodPressure","BMI"]
df = correlation_df(df = df_diabetes,
target_var = target,
feature_cols = inputs,
method = 'spearman')
df.show()
# Create a Spark session
spark = SparkSession.builder.appName("corr").getOrCreate()
# Get the Spark context
sc = spark.sparkContext
# Create a SQL context
sqlContext = SQLContext(sc)
# Create a Spark session
spark = SparkSession.builder.appName("corr").getOrCreate()
# Get the Spark context
sc = spark.sparkContext
# Create a SQL context
sqlContext = SQLContext(sc)
reticulate::repl_python()
